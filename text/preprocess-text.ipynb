{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prprocess custom text data using Torchtext\n",
    "\n",
    "Illustrates the usage of torchtext on a dataset that is not built-in. We will preprocess a dataset that can be further utilized to train a sequeence to sequence model for machine translation without using legacy version of torchtext.\n",
    "\n",
    "The notebook will cover:\n",
    "- read a dataset\n",
    "- Tokenize sentence\n",
    "- apply transformation to sentence\n",
    "- perform bucketing batching\n",
    "\n",
    "Link: https://pytorch.org/tutorials/beginner/torchtext_custom_dataset_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "de = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'deu.txt'\n",
    "data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode='rb')\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code block, we are doing following things:\n",
    "1. We are creating an iterable of filenames\n",
    "2. We pass the iterable to fileopener which then opens the files in read mode\n",
    "3. We call a function to parse the file, which again returns an iterable of tuples representing each rows of the tab delimited file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify if the iterable has the pair of sentences as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'Geh.', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)')\n"
     ]
    }
   ],
   "source": [
    "for sample in data_pipe:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that we also have attribution details along with pairs of sentences. We will write a small function to remove the attribution details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAttribution(row):\n",
    "    \"\"\"Function to keep the first element in a tumple\"\"\"\n",
    "    return row[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.map(removeAttribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verify the data_pipe only contains pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'Geh.')\n"
     ]
    }
   ],
   "source": [
    "for sample in data_pipe:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define a few function to perform tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engTokenize(text):\n",
    "    \"\"\"Tokenize an english text and return a list of tokens\"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "\n",
    "def deTokenize(text):\n",
    "    \"\"\"Tokenize a german text and return a list of tokens\"\"\"\n",
    "    return [token.text for token in de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out the functions above to confirm they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have', 'a', 'good', 'date', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(engTokenize(\"have a good date!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['haben', 'sie', 'einen', 'guten', 'tag', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(deTokenize(\"haben sie einen guten tag!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Voacbulary\n",
    "\n",
    "Lets us consider an english sentence as the source and german sentence as the target.\n",
    "\n",
    "Vocabulary can be considered as the set of unique words we have in the dataset. We will build vocabulary for both our source and target now.\n",
    "\n",
    "We neeed to define a function to get tokens from elements of tuples in the iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(data_iter, place):\n",
    "    \"\"\"\n",
    "    Function to yield tokens from an iterator. Since our iterator contains tuples of sentences\n",
    "    (sorce and target), \"place\" parameters defines for which index to return the token for. \n",
    "    \"place = 0\" for source and \"place=1\" for traget\n",
    "    \"\"\"\n",
    "    for english, german in data_iter:\n",
    "        if place == 0:\n",
    "            yield english(english)\n",
    "        else:\n",
    "            yield deTokenize(german)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build vocabulary for source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe, 2),\n",
    "    min_freq=2,\n",
    "    specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "source_vocab.set_default_index(source_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above builds the vocabulary from the iterator:\n",
    "1. getokens function places=0 as we need vocabulary for source sentences\n",
    "2. set min_feq=2 means the function will skip those words that occurs less than 2 times\n",
    "3. We specify some special tokens\n",
    "    - sos for the start of sentence\n",
    "    - eos for the end of sentence\n",
    "    - unk for unknown words\n",
    "    - pad is padding token\n",
    "4. set special_first=true which means pad will get index 0, sos will be index 1, eos index 2 and unk will get index 3 in the vocabulary\n",
    "5. set the default index as index of unk. If some words is not in the vocabulary we will use unk instead of that unknown word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will build vocabulary for target sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe, 1),\n",
    "    min_freq=2,\n",
    "    specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "target_vocab.set_default_index(target_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example above shows how we can add special tokens to our vocabulary. Special tokens may change based on the requirements.\n",
    "\n",
    "Now we can verify that special tokens are placed at the beginning and then othe words. in the below code reutrns a list which tokens at index based on vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '.', ',', 'Tom', 'Ich', '?']\n"
     ]
    }
   ],
   "source": [
    "print(source_vocab.get_itos()[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalize sentence using vocabulary\n",
    "\n",
    "After building the vocabulary, we need to convert our sentences to corresponding indcies.\n",
    "\n",
    "We will need additional functions for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. \n",
    "    The return transform is applied to sequence of tokens\n",
    "    \"\"\"\n",
    "    text_transform = T.Sequential(\n",
    "        # converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        # add sos at beginning of each sentence. 1 because of the index of sos in vocabulary is \n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        # add eos at beginning of each sentence. 2 because of the index of eos\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see how to use the above function. It returns an object of transforms which will use on our sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some sentence=I changed.\n",
      "Transform sentence=[1, 19362, 3, 4, 2]\n",
      "<sos> I <unk> . <eos> "
     ]
    }
   ],
   "source": [
    "temp_list = list(data_pipe)\n",
    "some_sentence = temp_list[798][0]\n",
    "print(\"some sentence=\", end=\"\")\n",
    "print(some_sentence)\n",
    "transform_sentence = getTransform(source_vocab)(engTokenize(some_sentence))\n",
    "print(\"Transform sentence=\", end=\"\")\n",
    "print(transform_sentence)\n",
    "index_to_string = source_vocab.get_itos()\n",
    "for index in transform_sentence:\n",
    "    print(index_to_string[index], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explaination above the above code:\n",
    "- take a source sentence from list that we create from data_pipe\n",
    "- we get a transform based on source vocabulary and apply it to a tokenized sentence\n",
    "    - transforms take list of words and not sentence\n",
    "- we get the mapping of index to string and then use it get the transformed sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will use datapipe functions to apply trnasform to all our sentences. We will need additional functions for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTransform(sequence_pair):\n",
    "    \"\"\" \n",
    "    apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "    return (\n",
    "        getTransform(source_vocab)(engTokenize(sequence_pair[0])),\n",
    "        getTransform(target_vocab)(deTokenize(sequence_pair[1]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 3, 4, 2], [1, 743, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "data_pipe = data_pipe.map(applyTransform) # apply the function to each elment\n",
    "temp_list = list(data_pipe)\n",
    "print(temp_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make batches (with bucket batch)\n",
    "\n",
    "Generally, we train models in batches. While working for sequence to sequence models, its recommended to keep the length of sequence in batch similar. \n",
    "\n",
    "For that we will use bucketbatch function of data pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortBucket(bucket):\n",
    "    \"\"\" \n",
    "    function to sort a given bucket. \n",
    "    Here we want to sort based on length of source and target sequence\n",
    "    \"\"\"\n",
    "    return sorted(bucket, key=lambda x: (len(x[0]), len([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply the bucketbatch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.bucketbatch(\n",
    "    batch_size = 4, batch_num=5,  bucket_num=1,\n",
    "    use_in_batch_shuffle=False, sort_key=sortBucket\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above code will:\n",
    "- keepp the batch size to 4\n",
    "- batch num is the number of batches to keep in a bucket\n",
    "- bucket num is the number of buckets to keep in a pool for shuffling\n",
    "- sort key specifices the function that takes a bucket and sorts it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us consider a batch of sorce sentences as x and a batch of targets sentences as y. Generally, while training a model, we predict on a batch of x and compare the result with y.\n",
    "\n",
    "But a batch in our data_pip is of the form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 3, 24, 2], [1, 3, 24, 2]), ([1, 3, 24, 2], [1, 1392, 10507, 24, 2]), ([1, 3, 4, 2], [1, 3, 24, 2]), ([1, 3, 24, 2], [1, 1704, 3774, 24, 2])]\n"
     ]
    }
   ],
   "source": [
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will convert them into the form ((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperateSourceTarget(sequence_pairs):\n",
    "    \"\"\" \n",
    "    input of form [(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]\n",
    "    output of form ((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))\n",
    "    \"\"\"\n",
    "    source, target = zip(*sequence_pairs)\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.map(seperateSourceTarget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(([1, 3, 3, 4, 2], [1, 19362, 3, 4, 2], [1, 19362, 3, 4, 2], [1, 19362, 5522, 4, 2]), ([1, 504, 500, 4, 2], [1, 7, 1946, 31, 4, 2], [1, 7, 22, 31, 1473, 4, 2], [1, 7, 1174, 4, 2]))\n"
     ]
    }
   ],
   "source": [
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding \n",
    "\n",
    "As mentioned earlier, while building vocabulary, we need to pad shorter sentences in a batch to maek all the sequences in a batch of equal length.\n",
    "\n",
    "We can perform padding as the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPadding(pair_of_sequences):\n",
    "    \"\"\" \n",
    "    convert sequences to tensors and apply padding\n",
    "    \"\"\"\n",
    "    return (T.ToTensor(0)(list(pair_of_sequences[0])), T.ToTensor(0)(list(pair_of_sequences[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.map(applyPadding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the index to string mapping to see how the sequences would look with tokens instead of indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_index_to_string = source_vocab.get_itos()\n",
    "target_index_to_string = target_vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showSomeTransformedSenteneces(data_pipe):\n",
    "    \"\"\" \n",
    "    function to show how the sentences look like after applying all transforms.\n",
    "    \"\"\"\n",
    "    for sources, targets in data_pipe:\n",
    "        if sources[0][-1] != 0:\n",
    "            continue # just to visualize padding\n",
    "        for i in range(4):\n",
    "            source = \"\"\n",
    "            for token in sources[i]:\n",
    "                source += \" \" + source_index_to_string[token]\n",
    "            target = \"\"\n",
    "            for token in targets[i]:\n",
    "                target += \" \" + target_index_to_string[token]\n",
    "            print(f\"Source: {source}\")\n",
    "            print(f\"Target: {target}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  <sos> <unk> ! <eos> <pad>\n",
      "Target:  <sos> Schnell ! <eos> <pad>\n",
      "Source:  <sos> <unk> . <eos> <pad>\n",
      "Target:  <sos> Entspann dich . <eos>\n",
      "Source:  <sos> <unk> ! <eos> <pad>\n",
      "Target:  <sos> Feuer ! <eos> <pad>\n",
      "Source:  <sos> <unk> <unk> . <eos>\n",
      "Target:  <sos> Tue es . <eos>\n"
     ]
    }
   ],
   "source": [
    "showSomeTransformedSenteneces(data_pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
